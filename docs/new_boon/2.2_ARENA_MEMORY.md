## Chapter 2.2: Arena Memory

### 2.2.0 Why Arena?

The new engine uses arena allocation instead of alternatives like `Arc<T>`, generational indices with indirection, or ECS frameworks. Here's why:

**1. Snapshot-ability (Primary Reason)**
The entire reactive graph can be serialized for persistence, debugging, and hot reload. Arena nodes are plain data with no hidden references or closures.

```rust
// With arena: trivial to serialize
let snapshot = arena.nodes.iter().map(|n| n.snapshot()).collect();
serde_json::to_string(&snapshot)?;

// With Arc<ValueActor>: closures, channels, tasks - not serializable
```

**2. Simplicity**
Direct SlotId indexing with generation check. No complex reference counting, weak references, or cyclic GC.

```rust
// Simple, predictable access
let node = arena.get(slot_id)?;
```

**3. Hardware Mental Model**
Maps naturally to the register/wire model. Nodes are like registers with fixed addresses. This aids reasoning about dataflow and enables future FPGA targeting.

**4. Cache Efficiency**
Nodes in contiguous memory. Iteration is cache-friendly. No pointer chasing like `Arc<T>` graphs.

**5. Debugging**
Single source of truth. Can dump entire arena state. No hidden async tasks or channel buffers to track.

**Alternatives Considered:**

| Approach | Why Not |
|----------|---------|
| `Arc<ValueActor>` | Hard to serialize, async complexity, debugging nightmare |
| Generational indices + indirection | Extra indirection layer, no clear benefit |
| ECS (specs, hecs) | Overkill for reactive graphs, different access patterns |
| Petgraph | Graph-centric, not reactive-centric |

---

### 2.2.1 SlotId (Generational Index)

Replace `Arc<ValueActor>` with arena slot references:

```rust
#[derive(Clone, Copy, PartialEq, Eq, Hash)]
pub struct SlotId {
    index: u32,
    generation: u32,
}
```

**Benefits:**
- No reference counting overhead
- Use-after-free detection via generation check
- Cache-friendly contiguous memory
- Serializable (just index + generation)

### 2.2.2 Arena Structure

```rust
pub struct Arena {
    nodes: Vec<ReactiveNode>,
    free_list: Vec<u32>,
    generation: u32,

    // NodeAddress storage for deterministic sorting (see §2.2.6)
    addresses: HashMap<SlotId, NodeAddress>,
}
```

**For multi-threading:** Each Web Worker gets its own arena. Cross-worker references use `(WorkerId, SlotId)`.

**Note:** `addresses` is stored separately from `ReactiveNode` to keep the hot path (node processing) compact. See §2.2.6 for rationale.

### 2.2.3 Arena Allocation & Resizing

**Initial Size:**
```rust
impl Arena {
    pub fn new() -> Self {
        Self {
            nodes: Vec::with_capacity(1024),  // Initial capacity: 1024 nodes
            free_list: Vec::new(),
            generation: 0,
        }
    }
}
```

**Allocation Strategy:**
1. **First:** Check free_list for recycled slots
2. **Then:** Allocate from end of nodes vector
3. **If full:** Grow arena (double capacity)

```rust
impl Arena {
    pub fn alloc(&mut self) -> SlotId {
        if let Some(index) = self.free_list.pop() {
            // Reuse freed slot, bump generation
            self.nodes[index as usize].generation += 1;
            SlotId { index, generation: self.nodes[index as usize].generation }
        } else {
            // Allocate new slot
            let index = self.nodes.len() as u32;
            self.nodes.push(ReactiveNode::default());
            SlotId { index, generation: 0 }
        }
    }

    pub fn free(&mut self, slot: SlotId) {
        if self.is_valid(slot) {
            self.free_list.push(slot.index);
            // Generation stays - next alloc will bump it
        }
    }
}
```

**Resizing Policy:**
| Scenario | Behavior |
|----------|----------|
| Arena full, need more | `Vec::push` auto-doubles capacity |
| Many freed slots | Reuse via free_list (no shrink) |
| Memory pressure (future) | Could compact + remap SlotIds |

#### Arena Compaction Strategy (Deferred)

**Current approach:** Accept fragmentation for simplicity. The free_list can grow unbounded, but:
- Freed slots are reused on next allocation
- Memory is bounded by peak usage, not total allocations
- For typical UI apps, fragmentation is negligible

**Future compaction heuristic (if needed):**

```rust
impl Arena {
    /// Compact arena when fragmentation exceeds threshold
    pub fn should_compact(&self) -> bool {
        // Compact if free_list > 50% of total nodes
        self.free_list.len() > self.nodes.len() / 2
    }

    pub fn compact(&mut self) -> SlotIdRemapping {
        // 1. Create new arena with only live nodes
        // 2. Build old_slot → new_slot mapping
        // 3. Update all internal references
        // 4. Return mapping for external references (snapshots, etc.)
        todo!("Implement when memory pressure is observed in practice")
    }
}
```

**When to trigger compaction:**
- After batch list removal (e.g., "clear all completed" in todo_mvc)
- On memory warning callback (mobile browsers)
- Never during active tick (only at tick boundary)

**Trade-off:** Compaction requires SlotId remapping which is complex. Accept fragmentation initially.

#### Memory Fragmentation Trade-offs

| Concern | Mitigation |
|---------|------------|
| free_list unbounded growth | Reuse on next alloc, bounded by peak |
| Wasted Vec capacity | No shrink, but allocation is fast |
| Cache unfriendly access | Free slots don't affect iteration order |
| OOM on mobile | Defer: add memory budget when targeting mobile |

**Decision:** Accept fragmentation for M1. Add compaction only if real-world apps show memory issues.

**Why not fixed size?**
- Static programs: Known node count, arena could be fixed
- Dynamic Lists: User adds todos → need new slots
- Trade-off: Start small (1024), grow as needed

**SharedArrayBuffer constraint (multi-threading):**

**IMPORTANT:** Node state is NOT shared between workers. Only message queues use SharedArrayBuffer (see Issue 22 for details).

```rust
// SharedArrayBuffer is for MESSAGE QUEUES ONLY, not node storage
// Nodes live in thread-local arenas - see §2.2.4 tiered approach
```

**Sizing heuristics:**
| App Complexity | Recommended Initial Capacity |
|----------------|------------------------------|
| Simple (counter) | 64 nodes |
| Medium (shopping_list) | 256 nodes |
| Complex (todo_mvc) | 1024 nodes |
| Large app | 4096+ nodes |

Could analyze AST at compile time to estimate node count for better initial sizing.

### 2.2.4 WASM Memory Strategy

**Key insight:** WASM memory uses lazy page commit - declaring large `maximum` doesn't consume RAM.

```
WASM Memory Layout:
┌─────────────────────────────────────────────────────────────┐
│  initial = 16MB (actually allocated)                        │
├─────────────────────────────────────────────────────────────┤
│  ... grows on demand via memory.grow() ...                  │
│  (OS commits pages only when written)                       │
├─────────────────────────────────────────────────────────────┤
│  maximum = 4GB (just a limit, no RAM cost)                  │
└─────────────────────────────────────────────────────────────┘
```

**Recommended WASM config:**
```toml
# In Cargo.toml or .cargo/config.toml
[target.wasm32-unknown-unknown]
rustflags = [
    "-C", "link-arg=--initial-memory=16777216",   # 16MB initial (256 pages)
    "-C", "link-arg=--max-memory=4294967296",     # 4GB maximum (phone is fine!)
]
```

**Why this works for both scenarios:**

| Scenario | Behavior |
|----------|----------|
| Simple website on phone | Uses ~1-2MB, rest is uncommitted virtual space |
| NovyWave with lots of data | Grows on demand up to 4GB, no OOM until truly full |

**Arena growth triggers `memory.grow()`:**
```rust
impl Arena {
    pub fn alloc(&mut self) -> SlotId {
        // When Vec needs to grow beyond current WASM memory...
        // Vec::push() internally calls memory.grow()
        // Browser/OS commits new pages lazily
        self.nodes.push(ReactiveNode::default());  // May trigger memory.grow()
        // ...
    }
}
```

**SharedArrayBuffer (multi-threading) - tiered approach (see Issue 22 for full details):**
```rust
// TIER 1: Shared region for cross-worker COMMUNICATION ONLY (no node state!)
pub struct SharedRegion {
    pub message_queues: [[AtomicU64; 1024]; MAX_WORKERS],  // SPSC queues
    pub dirty_bitmap: [AtomicU64; 1024],  // Cross-worker dirty notifications
    // NO shared node state - too complex for atomics
}

// TIER 2: Thread-local growable arena for ALL nodes
pub struct LocalArena {
    nodes: Vec<ReactiveNode>,   // Grows on demand, NOT shared
    free_list: Vec<u32>,
}

// Cross-worker node references via MESSAGES, not direct access
// See Issue 28 for lock-free constraint (browser kills page on block)
```

**Summary:**
- Set large `maximum` (4GB) - it's free, just virtual address space
- Start small `initial` (16MB) - actual RAM usage
- Arena `Vec` grows naturally, OS commits pages on-demand
- Simple sites use minimal RAM, heavy apps scale up automatically
- SharedArrayBuffer: use tiered approach (small shared + large local)

### 2.2.5 ReactiveNode (Hardware-Friendly)

Fixed-size node structure (fits cache line):

```rust
#[repr(C, align(64))]
pub struct ReactiveNode {
    // Hot data (8 bytes)
    generation: u32,
    version: u32,

    // Flags (4 bytes)
    dirty: bool,
    kind_tag: u8,
    input_count: u8,
    subscriber_count: u8,

    // Inline small arrays (48 bytes)
    inputs: [SlotId; 4],      // 32 bytes
    subscribers: [SlotId; 2], // 16 bytes

    // Extension for large nodes and value storage
    extension: Option<Box<NodeExtension>>,
}

/// Heap-allocated extension for data that doesn't fit in 64-byte node.
/// See §2.3.6 for value storage semantics.
pub struct NodeExtension {
    // Value storage (see §2.3.6 for semantics)
    pub current_value: Option<Payload>,   // Last scalar value
    pub pending_deltas: Vec<Payload>,     // Accumulated deltas this tick

    // Type-specific data
    pub kind_data: NodeKindData,          // Extended kind data

    // Overflow arrays (for nodes with >4 inputs or >2 subscribers)
    pub extra_inputs: Vec<SlotId>,
    pub extra_subscribers: Vec<SlotId>,
}
```

**Design rationale:**
- 64-byte inline node for cache-friendly hot-path iteration
- Extension box holds value storage and overflow data
- Most nodes (80%+) fit in inline arrays; complex nodes use extension
- Value storage (`current_value`, `pending_deltas`) always in extension

**FPGA mapping:** Each node = one register + combinational logic block.

### 2.2.5.1 NodeExtension Allocation Strategy

**When is extension allocated?**

Extension is allocated **lazily on first need**, not eagerly at node creation:

```rust
impl ReactiveNode {
    /// Get or create extension (lazy allocation)
    pub fn extension_mut(&mut self) -> &mut NodeExtension {
        self.extension.get_or_insert_with(NodeExtension::default)
    }

    /// Check if extension exists (without allocating)
    pub fn has_extension(&self) -> bool {
        self.extension.is_some()
    }
}

impl EventLoop {
    fn emit(&mut self, slot: SlotId, payload: Payload) {
        let node = self.arena.get_mut(slot)?;

        // Allocates extension on first emit (lazy)
        let ext = node.extension_mut();
        ext.current_value = Some(payload);
        // ...
    }

    fn add_subscriber(&mut self, slot: SlotId, subscriber: SlotId) {
        let node = self.arena.get_mut(slot)?;

        // Try inline array first
        if node.subscriber_count < 2 {
            node.subscribers[node.subscriber_count as usize] = subscriber;
            node.subscriber_count += 1;
        } else {
            // Overflow: allocate extension (lazy)
            let ext = node.extension_mut();
            ext.extra_subscribers.push(subscriber);
        }
    }
}
```

**Allocation triggers:**

| Event | Extension Allocated? |
|-------|---------------------|
| Node creation (`arena.alloc()`) | No - extension is None |
| First `emit()` call | Yes - stores `current_value` |
| First delta emit (ListDelta/ObjectDelta) | Yes - stores in `pending_deltas` |
| >4 inputs or >2 subscribers | Yes - stores in `extra_inputs`/`extra_subscribers` |
| Node with kind_data (Bus items, Router fields) | Yes - stores kind-specific data |

**Which node kinds typically need extensions?**

| Node Kind | Needs Extension? | Reason |
|-----------|------------------|--------|
| Producer | Yes (on first emit) | Stores initial/constant value |
| Wire | Maybe | Only if it emits (most pass through) |
| Router | Yes | Has fields, may have kind_data |
| Bus | Yes | Stores items map, alloc_site |
| Register (HOLD) | Yes | Stores `stored_value` |
| Combiner (LATEST) | Yes | Caches combined output |
| Transformer (THEN) | Maybe | Only if it emits |
| IOPad (LINK) | Maybe | Depends on event buffer needs |
| TextTemplate | Yes | Caches rendered output |

**Memory optimization:**
- Simple passthrough nodes (Wire, some Transformers) may never allocate extension
- Extension is ~100-200 bytes; only allocated when needed
- This keeps memory usage proportional to actual data storage, not node count

### 2.2.6 NodeAddress Storage (Deterministic Sorting)

ReactiveNode is kept at 64 bytes for cache efficiency, so NodeAddress is stored separately:

```rust
impl Arena {
    // Side table: SlotId → NodeAddress (for deterministic sorting)
    // Populated during AST → arena compilation
    addresses: HashMap<SlotId, NodeAddress>,

    pub fn get_address(&self, slot: SlotId) -> Option<&NodeAddress> {
        self.addresses.get(&slot)
    }

    pub fn alloc_with_address(&mut self, addr: NodeAddress) -> SlotId {
        let slot = self.alloc();
        self.addresses.insert(slot, addr);
        slot
    }
}
```

**Why side table instead of inline field:**
- ReactiveNode stays 64 bytes (cache-line aligned)
- Address only needed for sorting dirty nodes, not hot path
- HashMap lookup is O(1), acceptable for tick processing

**Used by:** EventLoop sorts dirty_nodes by `(source_id, scope_id, port)` for determinism (see §2.4.2).

**Files to modify:**
- `crates/boon/src/engine_v2/arena.rs` - Arena, SlotId, ReactiveNode types

---

